{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I try to solve the multi-class classification problem using simpler methods. First I vectorize the words in sentences using TF-IDF method, then I apply multiple models and compare their results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file_path = './data/sportoclanky.csv'\n",
    "assert os.path.exists(dataset_file_path)\n",
    "df = pd.read_csv(dataset_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labelencoder = LabelEncoder()\n",
    "df['category_enc'] = labelencoder.fit_transform(df['category'])\n",
    "no_classes = len(df['category_enc'].unique())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have found libraries `stop_words` and `simplelemma` that can help preprocess the Czech texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "\n",
    "stop_words = get_stop_words('cz')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can gently preprocess the corpora:\n",
    "1. Convert to lowercase and remove punctuations and characters and then strip;\n",
    "2. Remove the stop words to exclude them from polluting our future vectors;\n",
    "3. Lemmatize the words to make sure that similar words are \"grouped\" into their base form. Another option is to use stemming, it is a cheaper but less intelligent operation. In "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "import simplemma\n",
    "\n",
    "def stopword(string):\n",
    "    non_stop = [i for i in string.split() if i not in stop_words]\n",
    "    return ' '.join(non_stop)\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower() #lowercase text\n",
    "    text=text.strip()  #get rid of leading/trailing whitespace \n",
    "    text=re.compile('<.*?>').sub('', text) #Remove HTML tags/markups\n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  #Replace punctuation with space. Careful since punctuation can sometime be useful\n",
    "    text = re.sub('\\s+', ' ', text)  #Remove extra space and tabs\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) #[0-9] matches any digit (0 to 10000...)\n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) #matches any digit from 0 to 100000..., \\D matches non-digits\n",
    "    text = re.sub(r'\\s+',' ',text) #\\s matches any whitespace, \\s+ matches multiple whitespace, \\S matches non-whitespace \n",
    "    \n",
    "    return text\n",
    "\n",
    "def lemmatize(text):\n",
    "    lemmas = [simplemma.lemmatize(t, lang='cs') for t in text]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "def complete_preprocessing(string):\n",
    "    return lemmatize(stopword(preprocess(string)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make it a little simpler I concatenate the `rss_title` and `rss_perex` into one piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['rss_title'] + ' ' + df['rss_perex']\n",
    "df['clean_text'] = df['text'].apply(lambda x: complete_preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(df[\u001b[39m'\u001b[39m\u001b[39mclean_text\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[0;32m      3\u001b[0m                                                     df[\u001b[39m'\u001b[39m\u001b[39mcategory_enc\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[0;32m      4\u001b[0m                                                     stratify\u001b[39m=\u001b[39mdf[\u001b[39m'\u001b[39m\u001b[39mcategory_enc\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[0;32m      5\u001b[0m                                                     random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m,\n\u001b[0;32m      6\u001b[0m                                                     test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m)\n\u001b[0;32m      8\u001b[0m X_train, X_val, y_train, y_val \u001b[39m=\u001b[39m train_test_split(X_train, \n\u001b[0;32m      9\u001b[0m                                                     y_train, \n\u001b[0;32m     10\u001b[0m                                                     stratify\u001b[39m=\u001b[39my_train, \n\u001b[0;32m     11\u001b[0m                                                     random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m,\n\u001b[0;32m     12\u001b[0m                                                     test_size\u001b[39m=\u001b[39m\u001b[39m0.25\u001b[39m) \u001b[39m# 0.25 x 0.8 = 0.2   \u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['clean_text'], \n",
    "                                                    df['category_enc'], \n",
    "                                                    stratify=df['category_enc'], \n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.2)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, \n",
    "                                                    y_train, \n",
    "                                                    stratify=y_train, \n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.25) # 0.25 x 0.8 = 0.2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#TF-IDF\n",
    "# tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True, min_df=5,\n",
    "                        ngram_range=(1, 2))\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_val_vectors_tfidf = tfidf_vectorizer.transform(X_val)  # not applying fit() the TfidfVectorizer to the test data\n",
    "# to not allow changing the word-indices and weight based on the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jk\\Projects\\seznam_interview_challenge\\env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jk\\Projects\\seznam_interview_challenge\\env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jk\\Projects\\seznam_interview_challenge\\env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jk\\Projects\\seznam_interview_challenge\\env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\jk\\Projects\\seznam_interview_challenge\\env\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "models = [\n",
    "    RandomForestClassifier(n_estimators=400, max_depth=20, random_state=0),\n",
    "    LinearSVC(),\n",
    "    MultinomialNB(),\n",
    "    LogisticRegression(random_state=0, solver='saga', max_iter = 400),\n",
    "]\n",
    "\n",
    "# 5 Cross-validation\n",
    "CV = 5\n",
    "cv_df = pd.DataFrame(index=range(CV * len(models)))\n",
    "\n",
    "entries = []\n",
    "for model in models:\n",
    "  model_name = model.__class__.__name__\n",
    "  accuracies = cross_val_score(model, X_train_vectors_tfidf, y_train, scoring='accuracy', cv=CV)\n",
    "  for fold_idx, accuracy in enumerate(accuracies):\n",
    "    entries.append((model_name, fold_idx, accuracy))\n",
    "    \n",
    "cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mean Accuracy</th>\n",
       "      <th>Standard deviation</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model_name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LinearSVC</th>\n",
       "      <td>0.412586</td>\n",
       "      <td>0.000205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.411950</td>\n",
       "      <td>0.000384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MultinomialNB</th>\n",
       "      <td>0.412622</td>\n",
       "      <td>0.000033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.392325</td>\n",
       "      <td>0.002263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Mean Accuracy  Standard deviation\n",
       "model_name                                               \n",
       "LinearSVC                    0.412586            0.000205\n",
       "LogisticRegression           0.411950            0.000384\n",
       "MultinomialNB                0.412622            0.000033\n",
       "RandomForestClassifier       0.392325            0.002263"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_accuracy = cv_df.groupby('model_name').accuracy.mean()\n",
    "std_accuracy = cv_df.groupby('model_name').accuracy.std()\n",
    "\n",
    "acc = pd.concat([mean_accuracy, std_accuracy], axis= 1, \n",
    "          ignore_index=True)\n",
    "acc.columns = ['Mean Accuracy', 'Standard deviation']\n",
    "acc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, I could make the algorithms converge due to the time constraints (changing the solver and increasing the number of iterations for the case of Logistic Regression), the model produces only the trivial results by predicting the largest class. \n",
    "However, I believe that hyperparameter tuning might produce better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1)\n",
    "X_vectors_count = vectorizer.fit_transform(df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_vectors_count, \n",
    "                                                    df['category_enc'], \n",
    "                                                    stratify=df['category_enc'], \n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.2)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_vectors_count,\n",
    "                                                    df['category_enc'], \n",
    "                                                    stratify=df['category_enc'], \n",
    "                                                    random_state=42,\n",
    "                                                    test_size=0.25) # 0.25 x 0.8 = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(n_estimators=400)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=400)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(n_estimators=400)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=400)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's balanced accuracy: 0.04317505188830834\n",
      "Model's accuracy: 0.4066534795900018\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics  \n",
    "y_pred = clf.predict(X_val)\n",
    "print(f\"Model's balanced accuracy: {metrics.balanced_accuracy_score(y_val, y_pred)}\")\n",
    "print(f\"Model's accuracy: {metrics.accuracy_score(y_val, y_pred)}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, neither TF-IDF, nor count vectorization give satisfactory results. Further steps might include hyperparater tuning, or better using a classifier created for imbalanced datasets, such as `imblearn` or `catboost`.\n",
    "\n",
    "Another (quite desperate) attempt to improve results can be translating text to English (e.g. using DeepL) and then applying classic verified methods for working with text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "399ba6da13f1a0ab27358d540618ecfafd37a842103cd8305f5c1fbe569841be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
